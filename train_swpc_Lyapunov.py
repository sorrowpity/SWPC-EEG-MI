# train_swpc_Lyapunov.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
import os

# å°å…¥ä½ çš„æ¨¡å‹å’ŒåŠ è¼‰å™¨
from model import EEGNet
from ShallowConvNet_Lyapunov import ShallowConvNet
from EEG_cross_subject_loader_MI_resting import EEG_loader_resting
from EEG_cross_subject_loader_MI import EEG_loader
from visualizer import plot_training_history

# =================================================================
# I. é…ç½®åƒæ•¸ (é‡å°é­¯æ£’æ€§å„ªåŒ–)
# =================================================================
BATCH_SIZE = 16
EPOCHS = 500
LR = 0.0005
WEIGHT_DECAY = 0.01  # åŠ å¼· L2 æ­£å‰‡åŒ–
DROPOUT_RATE = 0.6  # <--- æ–°å¢é€™ä¸€è¡Œï¼Œå»ºè­°å¾ 0.5 æé«˜åˆ° 0.6
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
TEST_SUBJ = 1


# =================================================================
# æ–°å¢ï¼šæ ‡ç­¾å½’ä¸€åŒ–å‡½æ•°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
# =================================================================

import nolds  # è«‹ç¢ºä¿å·²å®‰è£ pip install nolds


def compute_lyapunov_labels(data_np):
    """
    data_np: (N, Channels, Time)
    è¿”å›æ­¸ä¸€åŒ–å¾Œçš„ Lyapunov æŒ‡æ•¸æ¨™ç±¤ (N, 1)
    """
    print(f"--- æ­£åœ¨è¨ˆç®— {len(data_np)} å€‹æ¨£æœ¬çš„ Lyapunov è¤‡é›œåº¦æ¨™ç±¤... ---")
    lyaps = []
    # ç‚ºäº†é€Ÿåº¦ï¼Œæˆ‘å€‘å°é€šé“å–å¹³å‡å¾Œè¨ˆç®—å–®é€šé“çš„æ··æ²Œåº¦
    for i in range(len(data_np)):
        signal = np.mean(data_np[i], axis=0)
        try:
            # 250é»è¼ƒçŸ­ï¼Œä½¿ç”¨ Rosenstein ç®—æ³•è¿‘ä¼¼
            l = nolds.lyap_r(signal, emb_dim=5, lag=None)
        except:
            l = 0.0
        lyaps.append(l)

    lyaps = np.array(lyaps).astype(np.float32)
    # æ­¸ä¸€åŒ–åˆ° 0-1 ä¹‹é–“ï¼Œæ–¹ä¾¿æ¨¡å‹å›æ­¸
    lyaps = (lyaps - lyaps.min()) / (lyaps.max() - lyaps.min() + 1e-6)
    return lyaps

def normalize_labels(labels):
    """
    å°†ä»»æ„æ•´æ•°æ ‡ç­¾å½’ä¸€åŒ–ä¸º 0 å¼€å§‹çš„è¿ç»­æ•´æ•°ï¼ˆ0,1,2,...ï¼‰
    ä¾‹å¦‚ï¼š[1,2] -> [0,1]ï¼Œ[-1,1] -> [0,1]ï¼Œ[2,3] -> [0,1]
    """
    unique_labels = np.unique(labels)
    label_map = {old: new for new, old in enumerate(unique_labels)}
    normalized_labels = np.array([label_map[old] for old in labels])
    print(f"æ ‡ç­¾æ˜ å°„: {label_map} (åŸå§‹æ ‡ç­¾: {unique_labels}, å½’ä¸€åŒ–å: {np.unique(normalized_labels)})")
    return normalized_labels


# =================================================================
# II. æ•¸æ“šå¢å¼· (è§£æ±ºçª—å£åç§»æ•æ„Ÿèˆ‡ç›²ç›®è‡ªä¿¡)
# =================================================================
def augment_batch(inputs, shift_limit=25, noise_level=0.01):
    """
    inputs shape: (Batch, 1, Channels, Time)
    """
    b, c, h, w = inputs.shape
    # 1. éš¨æ©Ÿæ™‚é–“å¹³ç§»
    for i in range(b):
        shift = np.random.randint(-shift_limit, shift_limit)
        if shift > 0:
            inputs[i, :, :, shift:] = inputs[i, :, :, :-shift]
            inputs[i, :, :, :shift] = 0
        elif shift < 0:
            inputs[i, :, :, :shift] = inputs[i, :, :, -shift:]
            inputs[i, :, :, shift:] = 0
    # 2. æ³¨å…¥éš¨æ©Ÿé«˜æ–¯å™ªè²
    noise = torch.randn_like(inputs) * noise_level
    return inputs + noise


# =================================================================
# III. æ ¸å¿ƒè¨“ç·´æµç¨‹ (å«é©—è­‰èˆ‡æ—©åœ)
# =================================================================
def train_process(model, train_loader, val_loader, criterion, optimizer, save_path, is_stage1=False):
    best_val_loss = float('inf')
    patience = 30
    counter = 0
    history = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}

    # --- æ–°å¢ï¼šå‹•åŠ›å­¸æå¤±å‡½æ•¸ ---
    mse_criterion = nn.MSELoss()
    alpha = 0.5  # æ··æ²Œåº¦é æ¸¬ä»»å‹™çš„æ¬Šé‡

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)

    for epoch in range(EPOCHS):
        model.train()
        t_loss, t_correct, t_total = 0, 0, 0

        # ä¿®æ”¹ï¼šå¢åŠ  lyap_labels çš„è§£åŒ…
        for batch in train_loader:
            if is_stage1:
                inputs, labels, lyap_labels = batch
                lyap_labels = lyap_labels.to(DEVICE)
            else:
                inputs, labels = batch

            inputs = (inputs - inputs.mean(dim=-1, keepdim=True)) / (inputs.std(dim=-1, keepdim=True) + 1e-6)
            inputs = inputs.unsqueeze(1).float().to(DEVICE)
            inputs = augment_batch(inputs, shift_limit=30, noise_level=0.02)
            labels = labels.long().to(DEVICE)

            optimizer.zero_grad()

            # --- æ ¸å¿ƒä¿®æ”¹ï¼šè™•ç†æ¨¡å‹é›™è¼¸å‡º ---
            if is_stage1:
                outputs, pred_lyap = model(inputs)  # ç²å–åˆ†é¡å’Œå›æ­¸çµæœ
                loss_cls = criterion(outputs, labels)
                loss_dyn = mse_criterion(pred_lyap.squeeze(), lyap_labels)
                loss = loss_cls + alpha * loss_dyn  # ç¸½ Loss
            else:
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

            t_loss += loss.item()
            _, pred = torch.max(outputs, 1)
            t_total += labels.size(0)
            t_correct += (pred == labels).sum().item()

        # --- é©—è­‰éšæ®µ (ä¸ä½¿ç”¨æ•¸æ“šå¢å¼·) ---
        model.eval()
        v_loss, v_correct, v_total = 0, 0, 0
        with torch.no_grad():
            for batch in val_loader:  # ä¿®æ”¹ï¼šä¸è¦ç›´æ¥ç”¨ inputs, labels
                # æ ¸å¿ƒä¿®å¤ï¼šè§£åŒ…é€»è¾‘å¿…é¡»ä¸è®­ç»ƒé˜¶æ®µä¸€è‡´
                if is_stage1:
                    inputs, labels, lyap_labels = batch
                    lyap_labels = lyap_labels.to(DEVICE)
                else:
                    inputs, labels = batch

                inputs = (inputs - inputs.mean(dim=-1, keepdim=True)) / (inputs.std(dim=-1, keepdim=True) + 1e-6)
                inputs = inputs.unsqueeze(1).float().to(DEVICE)
                labels = labels.long().to(DEVICE)

                # åŒæ ·å¤„ç†åŒè¾“å‡º
                if is_stage1:
                    outputs, pred_lyap = model(inputs)
                    loss_cls = criterion(outputs, labels)
                    loss_dyn = mse_criterion(pred_lyap.squeeze(), lyap_labels)
                    loss = loss_cls + alpha * loss_dyn
                else:
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)

                v_loss += loss.item()
                _, pred = torch.max(outputs, 1)
                v_total += labels.size(0)
                v_correct += (pred == labels).sum().item()

        # è¨˜éŒ„æ•¸æ“š
        epoch_t_acc = 100 * t_correct / t_total
        epoch_v_acc = 100 * v_correct / v_total
        epoch_v_loss = v_loss / len(val_loader)

        # ä¿®å¤ï¼šåªè°ƒç”¨ä¸€æ¬¡ scheduler.step()
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(epoch_v_loss)
        new_lr = optimizer.param_groups[0]['lr']

        # 2. ä¿®æ”¹è¨˜éŒ„éƒ¨åˆ† (ç´„ç¬¬ 102 è¡Œ)
        history['loss'].append(t_loss / len(train_loader))  # æ”¹ç‚º loss
        history['acc'].append(epoch_t_acc)  # æ”¹ç‚º acc
        history['val_loss'].append(epoch_v_loss)
        history['val_acc'].append(epoch_v_acc)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_v_loss < best_val_loss:
            best_val_loss = epoch_v_loss
            torch.save(model.state_dict(), save_path)
            counter = 0
        else:
            counter += 1

        if (epoch + 1) % 10 == 0:
            print(
                f"Epoch {epoch + 1:03d} | Train: {epoch_t_acc:5.1f}% | Val: {epoch_v_acc:5.1f}% | ValLoss: {epoch_v_loss:.4f}")

        if counter >= patience:
            print(f"æ—©åœè§¸ç™¼ï¼åœ¨ç¬¬ {epoch + 1} è¼ªåœæ­¢è¨“ç·´ã€‚")
            break

        if old_lr != new_lr:
            print(f"ğŸ“‰ å­¸ç¿’ç‡å¾ {old_lr:.6f} ä¸‹é™è‡³ {new_lr:.6f}")

    plot_training_history(history, title=save_path, save_path=f"{save_path}.png")
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}\n")


from scipy.linalg import inv, sqrtm


def compute_EA_matrix(X):
    """
    è¨ˆç®— EA å°é½ŠçŸ©é™£ R = mean(X * X^T)^(-1/2)
    X: (N_samples, Channels, TimePoints)
    """
    n_samples = X.shape[0]
    n_channels = X.shape[1]
    cov = np.zeros((n_channels, n_channels))
    for i in range(n_samples):
        # è¨ˆç®—å–®å€‹æ¨£æœ¬çš„å”æ–¹å·®: (C, T) @ (T, C) -> (C, C)
        trial_cov = np.dot(X[i], X[i].T) / X.shape[2]
        cov += trial_cov
    mean_cov = cov / n_samples
    # è¨ˆç®— R = mean_cov^(-1/2)
    R = inv(sqrtm(mean_cov)).real
    return R


def apply_EA_to_dataset(X, subj_indices):
    """
    æŒ‰è¢«è©¦åˆ†åˆ¥é€²è¡Œå°é½Š
    subj_indices: èˆ‡ X å°æ‡‰çš„è¢«è©¦ç·¨è™Ÿåˆ—è¡¨
    """
    X_aligned = np.zeros_like(X)
    unique_subjs = np.unique(subj_indices)
    print(f"--- åŸ·è¡Œ EA å°é½Šï¼Œç¸½è¨ˆ {len(unique_subjs)} å€‹è¢«è©¦ ---")
    for subj in unique_subjs:
        mask = (subj_indices == subj)
        # 1. æå–è©²è¢«è©¦æ‰€æœ‰æ•¸æ“šä¸¦è¨ˆç®—å°é½ŠçŸ©é™£
        R = compute_EA_matrix(X[mask])
        # 2. æ‡‰ç”¨å°é½Š: X_aligned = R * X
        X_aligned[mask] = np.matmul(R, X[mask])
    return X_aligned


# =================================================================
# IV. ä¸»å‡½æ•¸
# =================================================================
def main():
    # 1. è¨“ç·´éšæ®µ 1ï¼šé ç¯©é¸ (Rest vs MI)
    print("\n>>> Stage 1: Training Prescreening (Rest vs MI)")
    loader_rest = EEG_loader_resting(test_subj=TEST_SUBJ)

    # ã€æ·»åŠ  EA å€åŸŸã€‘
    # æ³¨æ„ï¼šå‡è¨­ä½ çš„ loader_rest æœ‰æä¾› train_subj æ¨™ç±¤ï¼Œå¦‚æœæ²’æœ‰ï¼Œ
    # é€™è£¡å¯ä»¥æš«æ™‚æŠŠæ‰€æœ‰ train_x çœ‹ä½œä¸€é«”ï¼ˆé›–ç„¶æ•ˆæœç•¥æ‰“æŠ˜æ‰£ï¼‰ï¼Œ
    # ç†æƒ³æƒ…æ³æ˜¯å‚³å…¥ loader_rest.train_subj
    train_x_ea = apply_EA_to_dataset(loader_rest.train_x, loader_rest.train_subj)

    # å¯¹ Stage 1 æ ‡ç­¾ä¹Ÿåšå½’ä¸€åŒ–ï¼ˆä¿é™©ï¼‰
    train_y_norm_1 = normalize_labels(loader_rest.train_y)

    # --- é—œéµï¼šåœ¨æ­¤è™•è¨ˆç®— Lyapunov æ¨™ç±¤ ---
    train_lyap = compute_lyapunov_labels(train_x_ea)

    # ä¿®æ”¹åŸæœ¬çš„ TensorDatasetï¼Œä½¿ç”¨ train_x_ea å’Œå½’ä¸€åŒ–åçš„æ ‡ç­¾
    # ä¿®æ”¹ TensorDatasetï¼Œå¤šå‚³å…¥ä¸€å€‹æ¨™ç±¤
    full_ds_1 = TensorDataset(
        torch.from_numpy(train_x_ea),
        torch.from_numpy(train_y_norm_1),
        torch.from_numpy(train_lyap)  # <--- æ–°å¢
    )

    # 80/20 åŠƒåˆ†
    train_size = int(0.8 * len(full_ds_1))
    val_size = len(full_ds_1) - train_size
    ds_train_1, ds_val_1 = random_split(full_ds_1, [train_size, val_size])

    train_loader_1 = DataLoader(ds_train_1, batch_size=BATCH_SIZE, shuffle=True)
    val_loader_1 = DataLoader(ds_val_1, batch_size=BATCH_SIZE)

    model_1 = ShallowConvNet(2, loader_rest.train_x.shape[1], loader_rest.train_x.shape[2], DROPOUT_RATE).to(DEVICE)
    # ä½¿ç”¨ Label Smoothing æŠ‘åˆ¶ç›²ç›®è‡ªä¿¡
    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)
    optimizer_1 = optim.Adam(model_1.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    train_process(model_1, train_loader_1, val_loader_1, criterion, optimizer_1, 'prescreen_model.pth', is_stage1=True)

    # --- åˆ†å‰²ç·š ---

    # 2. è¨“ç·´éšæ®µ 2ï¼šå·¦ vs å³
    print(">>> Stage 2: Training Classifier (Left vs Right)")
    loader_cls = EEG_loader(test_subj=TEST_SUBJ)

    if hasattr(loader_cls, 'train_subj'):
        train_x_ea_2 = apply_EA_to_dataset(loader_cls.train_x, loader_cls.train_subj)
    else:
        print("âš ï¸ è­¦å‘Š: Stage 2 åŠ è¼‰å™¨ç¼ºå°‘ train_subjï¼Œå°‡ä½¿ç”¨å…¨å±€ EA (æ•ˆæœè¼ƒå·®)")
        R_global = compute_EA_matrix(loader_cls.train_x)
        train_x_ea_2 = np.matmul(R_global, loader_cls.train_x)

    # æ ¸å¿ƒä¿®å¤ï¼šå½’ä¸€åŒ– Stage 2 çš„æ ‡ç­¾
    train_y_norm_2 = normalize_labels(loader_cls.train_y)

    full_ds_2 = TensorDataset(torch.from_numpy(train_x_ea_2), torch.from_numpy(train_y_norm_2))

    train_size_2 = int(0.8 * len(full_ds_2))
    val_size_2 = len(full_ds_2) - train_size_2
    ds_train_2, ds_val_2 = random_split(full_ds_2, [train_size_2, val_size_2])

    train_loader_2 = DataLoader(ds_train_2, batch_size=BATCH_SIZE, shuffle=True)
    val_loader_2 = DataLoader(ds_val_2, batch_size=BATCH_SIZE)

    model_2 = EEGNet(2, loader_cls.train_x.shape[1], loader_cls.train_x.shape[2], DROPOUT_RATE).to(DEVICE)
    optimizer_2 = optim.Adam(model_2.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    train_process(model_2, train_loader_2, val_loader_2, criterion, optimizer_2, 'classifier_model.pth')


if __name__ == '__main__':
    main()